{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378f7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                 time     gl\n",
      "0  183  2015-05-16 05:35:41  162.0\n",
      "1  183  2015-05-16 05:30:41  164.0\n",
      "2  183  2015-05-16 05:25:41  168.0\n",
      "3  183  2015-05-16 05:20:41  169.0\n",
      "4  183  2015-05-16 05:15:41  170.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r\"./Aleppo2017_processed.csv\",encoding='utf-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ea21dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([263,  77, 193, 277, 229, 245, 155, 251, 111, 164,\n",
      "       ...\n",
      "       128, 264, 186, 162, 249,  39,  52, 266, 223, 289],\n",
      "      dtype='int64', name='id', length=226)\n"
     ]
    }
   ],
   "source": [
    "ID = df['id'].value_counts().index\n",
    "print(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34216f5",
   "metadata": {},
   "source": [
    "# 将数据整合 \n",
    "\n",
    "## 先将每个受试者的数据提取，安装时间排序再分段，每500点为一段\n",
    "\n",
    "### for example  第一个受试者的数据窗口\n",
    "\n",
    "### 第一窗[0,1,2,3,4,......499] target = 500\n",
    "### 第二窗[1,2,3,4,5,......500] target = 501\n",
    "### 第三窗[2,3,4,5,6,......501] target = 502\n",
    "### 第四窗[3,4,5,6,7,......502] target = 503\n",
    "\n",
    "#### 那data_feature 内的数据为：\n",
    "[第一个受试者的数据：\n",
    "[0,1,2,3,4,......499],    #targets:[500]\n",
    "[1,2,3,4,5,......500],    #targets:[501]\n",
    "[2,3,4,5,6,......501],    #targets:[502]\n",
    "[3,4,5,6,7,......502],    #targets:[503]\n",
    "第二个受试者的数据：\n",
    "[0,1,2,3,4,......499],\n",
    "[1,2,3,4,5,......500],\n",
    "[2,3,4,5,6,......501],\n",
    "[3,4,5,6,7,......502]]\n",
    "\n",
    "#### 对每个窗口提取特征\n",
    "[第一个受试者的数据：\n",
    "[0,1,2,3,4,......499],    #feature:[均值, 标准差, 方差，协方差]  target = 500\n",
    "[1,2,3,4,5,......500],    #targets:[均值, 标准差, 方差，协方差]  target = 501\n",
    "[2,3,4,5,6,......501],    #targets:[均值, 标准差, 方差，协方差]  target = 502\n",
    "[3,4,5,6,7,......502],    #targets:[均值, 标准差, 方差，协方差]  target = 503\n",
    "第二个受试者的数据：\n",
    "[0,1,2,3,4,......499],\n",
    "[1,2,3,4,5,......500],\n",
    "[2,3,4,5,6,......501],\n",
    "[3,4,5,6,7,......502]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8df75162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_5652\\1675604409.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['time'] = pd.to_datetime(data['time'], format=\"%Y-%m-%d  %H:%M:%S\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94180\n"
     ]
    }
   ],
   "source": [
    "data_feature = []\n",
    "data_target = []\n",
    "seq = 500\n",
    "data = df[df['id'] == 263]\n",
    "#将time改成Date类数据类型\n",
    "data['time'] = pd.to_datetime(data['time'], format=\"%Y-%m-%d  %H:%M:%S\")\n",
    "#按照时间排序\n",
    "data = data.sort_values(by='time')   \n",
    "data['target'] = data['gl'].shift(-1)\n",
    "\n",
    "data.dropna()                      # 使用了shift函数，在最后必然是有缺失值的，这里去掉缺失值所在行\n",
    "data = data[['gl','target']].astype(np.float32)  # 修改数据类型\n",
    "\n",
    "for index in range(len(data) - seq):\n",
    "    # 构建特征集\n",
    "    data_feature.append(data['gl'][index: index + seq].values)\n",
    "    # 构建target集\n",
    "    data_target.append(data['target'][index:index + seq])\n",
    "print(len(data_feature))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8286bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id为 263的受试者的数据窗口加载完成\n",
      "id为 77的受试者的数据窗口加载完成\n",
      "id为 193的受试者的数据窗口加载完成\n",
      "id为 277的受试者的数据窗口加载完成\n",
      "id为 229的受试者的数据窗口加载完成\n",
      "id为 245的受试者的数据窗口加载完成\n",
      "id为 155的受试者的数据窗口加载完成\n",
      "id为 251的受试者的数据窗口加载完成\n",
      "id为 111的受试者的数据窗口加载完成\n",
      "id为 164的受试者的数据窗口加载完成\n",
      "id为 228的受试者的数据窗口加载完成\n",
      "id为 227的受试者的数据窗口加载完成\n",
      "id为 68的受试者的数据窗口加载完成\n",
      "id为 130的受试者的数据窗口加载完成\n",
      "id为 271的受试者的数据窗口加载完成\n",
      "id为 23的受试者的数据窗口加载完成\n",
      "id为 232的受试者的数据窗口加载完成\n",
      "id为 95的受试者的数据窗口加载完成\n",
      "id为 7的受试者的数据窗口加载完成\n",
      "id为 137的受试者的数据窗口加载完成\n",
      "id为 102的受试者的数据窗口加载完成\n",
      "id为 70的受试者的数据窗口加载完成\n",
      "id为 67的受试者的数据窗口加载完成\n",
      "id为 101的受试者的数据窗口加载完成\n",
      "id为 127的受试者的数据窗口加载完成\n",
      "id为 269的受试者的数据窗口加载完成\n",
      "id为 250的受试者的数据窗口加载完成\n",
      "id为 243的受试者的数据窗口加载完成\n",
      "id为 240的受试者的数据窗口加载完成\n",
      "id为 220的受试者的数据窗口加载完成\n",
      "id为 205的受试者的数据窗口加载完成\n",
      "id为 17的受试者的数据窗口加载完成\n",
      "id为 187的受试者的数据窗口加载完成\n",
      "id为 253的受试者的数据窗口加载完成\n",
      "id为 123的受试者的数据窗口加载完成\n",
      "id为 135的受试者的数据窗口加载完成\n",
      "id为 166的受试者的数据窗口加载完成\n",
      "id为 214的受试者的数据窗口加载完成\n",
      "id为 139的受试者的数据窗口加载完成\n",
      "id为 198的受试者的数据窗口加载完成\n",
      "id为 76的受试者的数据窗口加载完成\n",
      "id为 138的受试者的数据窗口加载完成\n",
      "id为 105的受试者的数据窗口加载完成\n",
      "id为 146的受试者的数据窗口加载完成\n",
      "id为 184的受试者的数据窗口加载完成\n",
      "id为 65的受试者的数据窗口加载完成\n",
      "id为 81的受试者的数据窗口加载完成\n",
      "id为 109的受试者的数据窗口加载完成\n",
      "id为 116的受试者的数据窗口加载完成\n",
      "id为 3的受试者的数据窗口加载完成\n",
      "id为 203的受试者的数据窗口加载完成\n",
      "id为 129的受试者的数据窗口加载完成\n",
      "id为 201的受试者的数据窗口加载完成\n",
      "id为 283的受试者的数据窗口加载完成\n",
      "id为 265的受试者的数据窗口加载完成\n",
      "id为 9的受试者的数据窗口加载完成\n",
      "id为 274的受试者的数据窗口加载完成\n",
      "id为 136的受试者的数据窗口加载完成\n",
      "id为 278的受试者的数据窗口加载完成\n",
      "id为 54的受试者的数据窗口加载完成\n",
      "id为 33的受试者的数据窗口加载完成\n",
      "id为 46的受试者的数据窗口加载完成\n",
      "id为 61的受试者的数据窗口加载完成\n",
      "id为 37的受试者的数据窗口加载完成\n",
      "id为 134的受试者的数据窗口加载完成\n",
      "id为 14的受试者的数据窗口加载完成\n",
      "id为 47的受试者的数据窗口加载完成\n",
      "id为 113的受试者的数据窗口加载完成\n",
      "id为 219的受试者的数据窗口加载完成\n",
      "id为 216的受试者的数据窗口加载完成\n",
      "id为 53的受试者的数据窗口加载完成\n",
      "id为 74的受试者的数据窗口加载完成\n",
      "id为 218的受试者的数据窗口加载完成\n",
      "id为 185的受试者的数据窗口加载完成\n",
      "id为 273的受试者的数据窗口加载完成\n",
      "id为 106的受试者的数据窗口加载完成\n",
      "id为 119的受试者的数据窗口加载完成\n",
      "id为 32的受试者的数据窗口加载完成\n",
      "id为 189的受试者的数据窗口加载完成\n",
      "id为 188的受试者的数据窗口加载完成\n",
      "id为 158的受试者的数据窗口加载完成\n",
      "id为 156的受试者的数据窗口加载完成\n",
      "id为 175的受试者的数据窗口加载完成\n",
      "id为 234的受试者的数据窗口加载完成\n",
      "id为 58的受试者的数据窗口加载完成\n",
      "id为 258的受试者的数据窗口加载完成\n",
      "id为 79的受试者的数据窗口加载完成\n",
      "id为 124的受试者的数据窗口加载完成\n",
      "id为 247的受试者的数据窗口加载完成\n",
      "id为 22的受试者的数据窗口加载完成\n",
      "id为 267的受试者的数据窗口加载完成\n",
      "id为 24的受试者的数据窗口加载完成\n",
      "id为 31的受试者的数据窗口加载完成\n",
      "id为 165的受试者的数据窗口加载完成\n",
      "id为 18的受试者的数据窗口加载完成\n",
      "id为 206的受试者的数据窗口加载完成\n",
      "id为 26的受试者的数据窗口加载完成\n",
      "id为 236的受试者的数据窗口加载完成\n",
      "id为 27的受试者的数据窗口加载完成\n",
      "id为 281的受试者的数据窗口加载完成\n",
      "id为 231的受试者的数据窗口加载完成\n",
      "id为 16的受试者的数据窗口加载完成\n",
      "id为 141的受试者的数据窗口加载完成\n",
      "id为 2的受试者的数据窗口加载完成\n",
      "id为 224的受试者的数据窗口加载完成\n",
      "id为 48的受试者的数据窗口加载完成\n",
      "id为 60的受试者的数据窗口加载完成\n",
      "id为 173的受试者的数据窗口加载完成\n",
      "id为 276的受试者的数据窗口加载完成\n",
      "id为 103的受试者的数据窗口加载完成\n",
      "id为 272的受试者的数据窗口加载完成\n",
      "id为 215的受试者的数据窗口加载完成\n",
      "id为 93的受试者的数据窗口加载完成\n",
      "id为 64的受试者的数据窗口加载完成\n",
      "id为 36的受试者的数据窗口加载完成\n",
      "id为 241的受试者的数据窗口加载完成\n",
      "id为 143的受试者的数据窗口加载完成\n",
      "id为 176的受试者的数据窗口加载完成\n",
      "id为 71的受试者的数据窗口加载完成\n",
      "id为 213的受试者的数据窗口加载完成\n",
      "id为 29的受试者的数据窗口加载完成\n",
      "id为 197的受试者的数据窗口加载完成\n",
      "id为 10的受试者的数据窗口加载完成\n",
      "id为 288的受试者的数据窗口加载完成\n",
      "id为 110的受试者的数据窗口加载完成\n",
      "id为 275的受试者的数据窗口加载完成\n",
      "id为 287的受试者的数据窗口加载完成\n",
      "id为 211的受试者的数据窗口加载完成\n",
      "id为 38的受试者的数据窗口加载完成\n",
      "id为 108的受试者的数据窗口加载完成\n",
      "id为 87的受试者的数据窗口加载完成\n",
      "id为 179的受试者的数据窗口加载完成\n",
      "id为 190的受试者的数据窗口加载完成\n",
      "id为 149的受试者的数据窗口加载完成\n",
      "id为 163的受试者的数据窗口加载完成\n",
      "id为 290的受试者的数据窗口加载完成\n",
      "id为 233的受试者的数据窗口加载完成\n",
      "id为 57的受试者的数据窗口加载完成\n",
      "id为 160的受试者的数据窗口加载完成\n",
      "id为 183的受试者的数据窗口加载完成\n",
      "id为 49的受试者的数据窗口加载完成\n",
      "id为 90的受试者的数据窗口加载完成\n",
      "id为 239的受试者的数据窗口加载完成\n",
      "id为 200的受试者的数据窗口加载完成\n",
      "id为 260的受试者的数据窗口加载完成\n",
      "id为 30的受试者的数据窗口加载完成\n",
      "id为 210的受试者的数据窗口加载完成\n",
      "id为 285的受试者的数据窗口加载完成\n",
      "id为 170的受试者的数据窗口加载完成\n",
      "id为 97的受试者的数据窗口加载完成\n",
      "id为 147的受试者的数据窗口加载完成\n",
      "id为 40的受试者的数据窗口加载完成\n",
      "id为 140的受试者的数据窗口加载完成\n",
      "id为 82的受试者的数据窗口加载完成\n",
      "id为 172的受试者的数据窗口加载完成\n",
      "id为 132的受试者的数据窗口加载完成\n",
      "id为 72的受试者的数据窗口加载完成\n",
      "id为 291的受试者的数据窗口加载完成\n",
      "id为 222的受试者的数据窗口加载完成\n",
      "id为 235的受试者的数据窗口加载完成\n",
      "id为 174的受试者的数据窗口加载完成\n",
      "id为 41的受试者的数据窗口加载完成\n",
      "id为 45的受试者的数据窗口加载完成\n",
      "id为 15的受试者的数据窗口加载完成\n",
      "id为 35的受试者的数据窗口加载完成\n",
      "id为 96的受试者的数据窗口加载完成\n",
      "id为 152的受试者的数据窗口加载完成\n",
      "id为 148的受试者的数据窗口加载完成\n",
      "id为 217的受试者的数据窗口加载完成\n",
      "id为 50的受试者的数据窗口加载完成\n",
      "id为 98的受试者的数据窗口加载完成\n",
      "id为 43的受试者的数据窗口加载完成\n",
      "id为 292的受试者的数据窗口加载完成\n",
      "id为 121的受试者的数据窗口加载完成\n",
      "id为 171的受试者的数据窗口加载完成\n",
      "id为 115的受试者的数据窗口加载完成\n",
      "id为 168的受试者的数据窗口加载完成\n",
      "id为 293的受试者的数据窗口加载完成\n",
      "id为 145的受试者的数据窗口加载完成\n",
      "id为 11的受试者的数据窗口加载完成\n",
      "id为 246的受试者的数据窗口加载完成\n",
      "id为 257的受试者的数据窗口加载完成\n",
      "id为 221的受试者的数据窗口加载完成\n",
      "id为 131的受试者的数据窗口加载完成\n",
      "id为 280的受试者的数据窗口加载完成\n",
      "id为 204的受试者的数据窗口加载完成\n",
      "id为 19的受试者的数据窗口加载完成\n",
      "id为 89的受试者的数据窗口加载完成\n",
      "id为 181的受试者的数据窗口加载完成\n",
      "id为 248的受试者的数据窗口加载完成\n",
      "id为 20的受试者的数据窗口加载完成\n",
      "id为 157的受试者的数据窗口加载完成\n",
      "id为 62的受试者的数据窗口加载完成\n",
      "id为 91的受试者的数据窗口加载完成\n",
      "id为 42的受试者的数据窗口加载完成\n",
      "id为 169的受试者的数据窗口加载完成\n",
      "id为 254的受试者的数据窗口加载完成\n",
      "id为 226的受试者的数据窗口加载完成\n",
      "id为 284的受试者的数据窗口加载完成\n",
      "id为 256的受试者的数据窗口加载完成\n",
      "id为 78的受试者的数据窗口加载完成\n",
      "id为 112的受试者的数据窗口加载完成\n",
      "id为 86的受试者的数据窗口加载完成\n",
      "id为 244的受试者的数据窗口加载完成\n",
      "id为 73的受试者的数据窗口加载完成\n",
      "id为 69的受试者的数据窗口加载完成\n",
      "id为 252的受试者的数据窗口加载完成\n",
      "id为 5的受试者的数据窗口加载完成\n",
      "id为 55的受试者的数据窗口加载完成\n",
      "id为 177的受试者的数据窗口加载完成\n",
      "id为 21的受试者的数据窗口加载完成\n",
      "id为 8的受试者的数据窗口加载完成\n",
      "id为 118的受试者的数据窗口加载完成\n",
      "id为 80的受试者的数据窗口加载完成\n",
      "id为 209的受试者的数据窗口加载完成\n",
      "id为 167的受试者的数据窗口加载完成\n",
      "id为 128的受试者的数据窗口加载完成\n",
      "id为 264的受试者的数据窗口加载完成\n",
      "id为 186的受试者的数据窗口加载完成\n",
      "id为 162的受试者的数据窗口加载完成\n",
      "id为 249的受试者的数据窗口加载完成\n",
      "id为 39的受试者的数据窗口加载完成\n",
      "id为 52的受试者的数据窗口加载完成\n",
      "id为 266的受试者的数据窗口加载完成\n",
      "id为 223的受试者的数据窗口加载完成\n",
      "id为 289的受试者的数据窗口加载完成\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "data_path = r\"./Aleppo2017_processed.csv\"\n",
    "class myData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(data_path,encoding='utf-8')\n",
    "        # 将所有被试的特征放入data_feature,将所有被试的标签放入data_targets\n",
    "        self.all_data_feature = []\n",
    "        self.all_data_target = []\n",
    "        self.data_feature = []\n",
    "        self.data_target = []\n",
    "        self.ID = self.df['id'].value_counts().index\n",
    "        seq = 1000    #这个也算超参数之一\n",
    "        for i in self.ID:\n",
    "            self.data = self.df[self.df['id'] == i].copy()\n",
    "            self.data = self.data.sort_values(by='time')\n",
    "            # 时间为行索引\n",
    "            self.data.index = self.data['time']\n",
    "\n",
    "            # 设置标签\n",
    "            self.data['target'] = self.data['gl'].shift(-1)\n",
    "            self.data.dropna()  # 使用了shift函数，在最后必然是有缺失值的，这里去掉缺失值所在行\n",
    "            self.data = self.data[['gl', 'target']].astype(np.float32)  # 修改数据类型\n",
    "            self.data = self.data[:16439]    #所有受试者中血糖数据个数最小 为16439\n",
    "\n",
    "            # 划分数据窗口\n",
    "           \n",
    "            for index in range(len(self.data) - seq):\n",
    "                # 构建特征集\n",
    "                self.data_feature.append(self.data['gl'][index: index + seq].values)\n",
    "                # 构建target集\n",
    "                self.data_target.append(self.data['target'].iloc[index + seq])\n",
    "            self.all_data_feature.append(self.data_feature)\n",
    "            self.all_data_target.append(self.data_target)\n",
    "            self.data_feature = []\n",
    "            self.data_target = []\n",
    "            print(\"id为 {}的受试者的数据窗口加载完成\".format(i))\n",
    "        self.all_data_feature = torch.tensor(self.all_data_feature)\n",
    "        self.all_data_target = torch.tensor(self.all_data_target)\n",
    "        \n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        datas = self.all_data_feature[idx]\n",
    "        targets = self.all_data_target[idx]\n",
    "        return datas, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ID)\n",
    "\n",
    "allData = myData()\n",
    "# 自动调用__getitem__( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e83deae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8316873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15439, 1000])\n",
      "torch.Size([15439])\n"
     ]
    }
   ],
   "source": [
    "datas,targets = allData[0]\n",
    "print(datas.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9bfc680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15439, 1000])\n",
      "torch.Size([15439])\n",
      "tensor([[157., 166., 172.,  ..., 192., 188., 183.],\n",
      "        [166., 172., 182.,  ..., 188., 183., 178.],\n",
      "        [172., 182., 184.,  ..., 183., 178., 173.],\n",
      "        ...,\n",
      "        [147., 143., 134.,  ..., 138., 132., 127.],\n",
      "        [143., 134., 123.,  ..., 132., 127., 123.],\n",
      "        [134., 123., 122.,  ..., 127., 123., 109.]])\n",
      "tensor([173., 169., 161.,  ..., 109., 112., 118.])\n"
     ]
    }
   ],
   "source": [
    "data2, target2 = allData[1]\n",
    "print(data2.shape)\n",
    "print(target2.shape)\n",
    "print(data2)\n",
    "print(target2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32a45af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fea9a78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "180\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "# 定义数据集大小\n",
    "dataset_size = len(allData)\n",
    "print(dataset_size)\n",
    "# 计算划分的数据集大小\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(allData, [train_size, test_size])\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53389075",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(allData, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e38c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([32, 15439, 1000])\n",
      "torch.Size([32, 15439])\n",
      "torch.Size([2, 15439, 1000])\n",
      "torch.Size([2, 15439])\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "\n",
    "# for inputs, targets in test_loader:\n",
    "#     print(inputs.shape)\n",
    "    # 在这里你可以对inputs和targets进行操作，它们已经被转换为张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5dc75",
   "metadata": {},
   "source": [
    "### 改动dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825dfcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id为 263的受试者的数据窗口加载完成\n",
      "id为 77的受试者的数据窗口加载完成\n",
      "id为 193的受试者的数据窗口加载完成\n",
      "id为 277的受试者的数据窗口加载完成\n",
      "id为 229的受试者的数据窗口加载完成\n",
      "id为 245的受试者的数据窗口加载完成\n",
      "id为 155的受试者的数据窗口加载完成\n",
      "id为 251的受试者的数据窗口加载完成\n",
      "id为 111的受试者的数据窗口加载完成\n",
      "id为 164的受试者的数据窗口加载完成\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "data_path = r\"./Aleppo2017_processed.csv\"\n",
    "class myData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(data_path,encoding='utf-8')\n",
    "        # 将所有被试的特征放入data_feature,将所有被试的标签放入data_targets\n",
    "        \n",
    "        self.all_data_feature = []\n",
    "        self.all_data_target = []\n",
    "        self.data_feature = []\n",
    "        self.data_target = []\n",
    "        self.ID = self.df['id'].value_counts().index\n",
    "        self.all_data_feat = torch.zeros(len(self.ID[:10]),15439,7)    #bs,seq,feat_size\n",
    "        seq = 1000    #这个也算超参数之一\n",
    "        for i in self.ID[:10]:\n",
    "            self.data = self.df[self.df['id'] == i].copy()\n",
    "            self.data = self.data.sort_values(by='time')\n",
    "            # 时间为行索引\n",
    "            self.data.index = self.data['time']\n",
    "\n",
    "            # 设置标签\n",
    "            self.data['target'] = self.data['gl'].shift(-1)\n",
    "            self.data.dropna()  # 使用了shift函数，在最后必然是有缺失值的，这里去掉缺失值所在行\n",
    "            self.data = self.data[['gl', 'target']].astype(np.float32)  # 修改数据类型\n",
    "            self.data = self.data[:16439]    #所有受试者中血糖数据个数最小 为16439\n",
    "\n",
    "            # 划分数据窗口\n",
    "           \n",
    "            for index in range(len(self.data) - seq):\n",
    "                # 构建特征集\n",
    "                self.data_feature.append(self.data['gl'][index: index + seq].values)\n",
    "                # 构建target集\n",
    "                self.data_target.append(self.data['target'].iloc[index + seq])\n",
    "            self.all_data_feature.append(self.data_feature)\n",
    "            self.all_data_target.append(self.data_target)\n",
    "            self.data_feature = []\n",
    "            self.data_target = []\n",
    "            print(\"id为 {}的受试者的数据窗口加载完成\".format(i))\n",
    "        #将数据类型转化为torch.Tensor\n",
    "        self.all_data_feature = torch.tensor(self.all_data_feature)\n",
    "        self.all_data_target = torch.tensor(self.all_data_target)\n",
    "        \n",
    "        #每个窗口有x个特征   \n",
    "        #计算每个窗口的mean\n",
    "        self.all_data_feat[:,:,0] = torch.mean(self.all_data_feature,dim=2)\n",
    "        #计算每个窗口的max\n",
    "        self.all_data_feat[:,:,1] = torch.max(self.all_data_feature,dim=2)[0]\n",
    "        #计算每个窗口的min\n",
    "        self.all_data_feat[:,:,2] = torch.min(self.all_data_feature,dim=2)[0]\n",
    "        #计算每个窗口的标准差\n",
    "        self.all_data_feat[:,:,3] = torch.std(self.all_data_feature,dim=2)\n",
    "        \n",
    "        #计算每个窗口的斜率\n",
    "#         x = torch.arange(1000, dtype=torch.float32)\n",
    "#         self.all_data_feat[:,:,4] = torch.plotfit(x,self.all_data_feature,deg=1)[0]\n",
    "\n",
    "        #计算每个窗口的四分位数\n",
    "        self.all_data_feat[:,:,4] = torch.quantile(self.all_data_feature, 0.25, dim=2)\n",
    "        self.all_data_feat[:,:,5] = torch.quantile(self.all_data_feature, 0.5, dim=2)\n",
    "        self.all_data_feat[:,:,6] = torch.quantile(self.all_data_feature, 0.75, dim=2)\n",
    "        #计算每个窗口的时间特征\n",
    "        #pass\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        datas = self.all_data_feat[idx]\n",
    "        targets = self.all_data_target[idx]\n",
    "        return datas, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ID[:10])\n",
    "\n",
    "allData = myData()\n",
    "# 自动调用__getitem__( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e866b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[192.4980, 344.0000,  52.0000,  ..., 142.0000, 198.0000, 242.0000],\n",
      "        [192.6570, 344.0000,  52.0000,  ..., 142.0000, 198.0000, 242.0000],\n",
      "        [192.8150, 344.0000,  52.0000,  ..., 142.7500, 198.5000, 242.0000],\n",
      "        ...,\n",
      "        [162.9530, 398.0000,  45.0000,  ..., 116.7500, 156.0000, 209.0000],\n",
      "        [162.9440, 398.0000,  45.0000,  ..., 116.7500, 156.0000, 209.0000],\n",
      "        [162.9320, 398.0000,  45.0000,  ..., 116.0000, 156.0000, 209.0000]])\n",
      "torch.Size([15439, 7])\n",
      "tensor([245., 248., 249.,  ..., 116., 118., 127.])\n",
      "torch.Size([15439])\n"
     ]
    }
   ],
   "source": [
    "data1,target1 = allData[0]\n",
    "print(data1)\n",
    "print(data1.shape)\n",
    "print(target1)\n",
    "print(target1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b8d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 8\n",
      "测试集大小 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "dataset_size = len(allData)\n",
    "\n",
    "# 计算划分的数据集大小\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# 使用 random_split 方法来随机划分数据集\n",
    "train_dataset, test_dataset = random_split(allData, [train_size, test_size])\n",
    "print(\"训练集大小：\", len(train_dataset))\n",
    "print(\"测试集大小\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba0c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =2\n",
    "# train = TensorDataset(trainX,trainY)\n",
    "# test = TensorDataset(testX,testY)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2f1c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15439, 7])\n",
      "torch.Size([2, 15439])\n",
      "torch.Size([2, 15439, 7])\n",
      "torch.Size([2, 15439])\n",
      "torch.Size([2, 15439, 7])\n",
      "torch.Size([2, 15439])\n",
      "torch.Size([2, 15439, 7])\n",
      "torch.Size([2, 15439])\n"
     ]
    }
   ],
   "source": [
    "for datas, targets in train_loader:\n",
    "    print(datas.shape)\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1e4cf",
   "metadata": {},
   "source": [
    "\n",
    "## 先将每个受试者的数据提取，安装时间排序再分段，每1000点为一段\n",
    "\n",
    "### for example  第一个受试者的数据窗口\n",
    "\n",
    "### 第一窗[0,1,2,3,4,......999] target = 1000\n",
    "### 第二窗[1,2,3,4,5,......1000] target = 1001\n",
    "### 第三窗[2,3,4,5,6,......1001] target = 1002\n",
    "### 第四窗[3,4,5,6,7,......1002] target = 1003\n",
    "\n",
    "#### 那data_feature 内的数据为：\n",
    "[第一个受试者的数据：\n",
    "[0,1,2,3,4,......999],    #targets:[1000]\n",
    "[1,2,3,4,5,......1000],    #targets:[1001]\n",
    "[2,3,4,5,6,......1001],    #targets:[1002]\n",
    "[3,4,5,6,7,......1002],    #targets:[1003]\n",
    "第二个受试者的数据：\n",
    "[0,1,2,3,4,......999],\n",
    "[1,2,3,4,5,......1000],\n",
    "[2,3,4,5,6,......1001],\n",
    "[3,4,5,6,7,......1002]]\n",
    "\n",
    "#### 对每个窗口提取特征\n",
    "[第一个受试者的数据：\n",
    "[0,1,2,3,4,......999],  ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]  target = 1000\n",
    "[1,2,3,4,5,......1000], ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]  target = 1001\n",
    "[2,3,4,5,6,......1001], ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]  target = 1002\n",
    "[3,4,5,6,7,......1002], ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]  target = 1003\n",
    "第二个受试者的数据：\n",
    "[0,1,2,3,4,......999],\n",
    "[1,2,3,4,5,......1000],\n",
    "[2,3,4,5,6,......1001],\n",
    "[3,4,5,6,7,......1002]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b256321",
   "metadata": {},
   "source": [
    "#### 对每个窗口提取特征\n",
    "[第一个受试者的数据：\n",
    "[0,1,2,3,4,......999],  ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[1,2,3,4,5,......1000], ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[2,3,4,5,6,......1001], ==>(特征提取)    #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[3,4,5,6,7,......1002],  ==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[4,5,6,7,8,......1003],  ==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[5,6,7,8,9,......1004],  ==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[6,7,8,9,10,......1005],  ==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[7,8,9,10,11,......1006], ==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数] \n",
    "[8,9,10,11,12,......1007],==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数]\n",
    "[9,10,11,12,13......1008],==>(特征提取)   #feature:[均值, 标准差, 方差，最大值，最小值，四分位数],target = 1009]\n",
    "<!-- 以此类推 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c99c690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                 time     gl\n",
      "0  183  2015-05-16 05:35:41  162.0\n",
      "1  183  2015-05-16 05:30:41  164.0\n",
      "2  183  2015-05-16 05:25:41  168.0\n",
      "3  183  2015-05-16 05:20:41  169.0\n",
      "4  183  2015-05-16 05:15:41  170.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(r\"./Aleppo2017_processed.csv\",encoding='utf-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85d00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda:0\")\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(myLSTM, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Readout layer 在LSTM后再加一个全连接层，因为是回归问题，所以不能在线性层后加激活函数\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        # 这里x.size(0)就是batch_size\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc(hn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fae1737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_path = \"./Aleppo2017_processed.csv\"\n",
    "class myData(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df = pd.read_csv(data_path, encoding='utf-8')\n",
    "        \n",
    "        self.data_feature = []\n",
    "        self.data_target = []\n",
    "        self.ID = self.df['id'].value_counts().index\n",
    "        # self.all_data_feat = torch.zeros(len(self.ID[:100]), 15439, 7)  # bs,seq,feat_size\n",
    "        seq = 288  # 这个也算超参数之一\n",
    "        for i in self.ID[:10]:\n",
    "            self.data = self.df[self.df['id'] == i].copy()\n",
    "            self.data = self.data.sort_values(by='time')\n",
    "            # 时间为行索引\n",
    "            self.data.index = self.data['time']\n",
    "\n",
    "            # 设置标签\n",
    "            self.data['target'] = self.data['gl'].shift(-1)\n",
    "            self.data.dropna()  # 使用了shift函数，在最后必然是有缺失值的，这里去掉缺失值所在行\n",
    "            self.data = self.data[['gl', 'target']].astype(np.float32)  # 修改数据类型\n",
    "            self.data = self.data[:2000]  # 所有受试者中血糖数据个数最小 为16439\n",
    "            self.data = self.data.gl\n",
    "            for j in range(2000 - seq):\n",
    "                window = self.data.iloc[j:j+seq].values\n",
    "                target = self.data.iloc[j+seq]\n",
    "                self.data_feature.append(window)\n",
    "                self.data_target.append(target)\n",
    "\n",
    "        self.data_feature = np.array(self.data_feature)\n",
    "        self.data_target = np.array(self.data_target)\n",
    "\n",
    "        # 转换为 PyTorch 张量\n",
    "        self.data_feature = torch.tensor(self.data_feature, dtype=torch.float32)\n",
    "        self.data_target = torch.tensor(self.data_target, dtype=torch.float32)\n",
    "        print(self.data_feature.shape)\n",
    "        print(self.data_target.shape)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        datas = self.data_feature[idx]\n",
    "        targets = self.data_target[idx]\n",
    "        return datas, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdeacadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9798458c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([263,  77, 193, 277, 229, 245, 155, 251, 111, 164,\n",
       "       ...\n",
       "       128, 264, 186, 162, 249,  39,  52, 266, 223, 289],\n",
       "      dtype='int64', name='id', length=226)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID = df['id'].value_counts().index\n",
    "ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9359ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        gl  target\n",
      "time                              \n",
      "2015-04-21 23:17:27  187.0   189.0\n",
      "2015-04-21 23:22:27  189.0   189.0\n",
      "2015-04-21 23:27:27  189.0   189.0\n",
      "2015-04-21 23:32:27  189.0   185.0\n",
      "2015-04-21 23:37:27  185.0   183.0\n",
      "...                    ...     ...\n",
      "2015-04-29 00:46:58  170.0   167.0\n",
      "2015-04-29 00:51:56  167.0   171.0\n",
      "2015-04-29 00:56:56  171.0   171.0\n",
      "2015-04-29 01:01:57  171.0   170.0\n",
      "2015-04-29 01:06:56  170.0   170.0\n",
      "\n",
      "[2000 rows x 2 columns]\n",
      "[187. 189. 189. 189. 185. 183. 183. 185. 185. 186.]\n",
      "<class 'numpy.ndarray'>\n",
      "190.0\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "i = 183\n",
    "data = df[df['id'] == i].copy()\n",
    "data = data.sort_values(by='time')\n",
    "# 时间为行索引\n",
    "data.index = data['time']\n",
    "\n",
    "# 设置标签\n",
    "data['target'] = data['gl'].shift(-1)\n",
    "data.dropna()  # 使用了shift函数，在最后必然是有缺失值的，这里去掉缺失值所在行\n",
    "data = data[['gl', 'target']].astype(np.float32)  # 修改数据类型\n",
    "data = data[:2000]  # 所有受试者中血糖数据个数最小 为16439\n",
    "print(data)\n",
    "data = data.gl\n",
    "print(data.iloc[0:10].values)\n",
    "print(type(data.iloc[0:10].values))\n",
    "print(data[10])\n",
    "print(type(data[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45e5d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17120, 288])\n",
      "torch.Size([17120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([288])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData = myData()\n",
    "# 自动调用__getitem__( )\n",
    "datas, targets = allData[0]\n",
    "datas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "492c0651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288, 2])\n"
     ]
    }
   ],
   "source": [
    "data1, target1 = allData[1]\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b967b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 8\n",
      "测试集大小 2\n"
     ]
    }
   ],
   "source": [
    "# 5、划分训练集和测试集\n",
    "# 定义数据集大小\n",
    "dataset_size = len(allData)\n",
    "\n",
    "# 计算划分的数据集大小\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# 使用 random_split 方法来随机划分数据集\n",
    "train_dataset, test_dataset = random_split(allData, [train_size, test_size])\n",
    "print(\"训练集大小：\", len(train_dataset))\n",
    "print(\"测试集大小\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7f371d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "# train = TensorDataset(trainX,trainY)\n",
    "# test = TensorDataset(testX,testY)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc452e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1  # 数据的特征数\n",
    "hidden_dim = 32  # 隐藏层的神经元个数\n",
    "num_layers = 2  # LSTM的层数\n",
    "output_dim = 1  # 预测值的特征数\n",
    "# （这是预测股票价格，所以这里特征数是1，如果预测一个单词，那么这里是one-hot向量的编码长度）\n",
    "\n",
    "# 实例化模型\n",
    "model = myLSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "if torch.cuda.is_available():\n",
    "    allen = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc8ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器和损失函数\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')  # 使用均方误差作为损失函数\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)  # 使用Adam优化算法\n",
    "if torch.cuda.is_available():\n",
    "    loss_fn = loss_fn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "988c67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定数据遍历次数\n",
    "num_epochs = 300\n",
    "# 记录训练次数\n",
    "total_train_step = 0\n",
    "# 记录测试次数\n",
    "total_test_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a3b5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1712, 288])\n",
      "torch.Size([1, 1712])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 6, got 288",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(trains\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(targets\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 8\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrains\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\AnacondaEnv\\envs\\pytorch02\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     16\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m---> 17\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\AnacondaEnv\\envs\\pytorch02\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\AnacondaEnv\\envs\\pytorch02\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mD:\\AnacondaEnv\\envs\\pytorch02\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\AnacondaEnv\\envs\\pytorch02\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 6, got 288"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    trains, targets = data\n",
    "    if torch.cuda.is_available():\n",
    "        trains = trains.cuda()\n",
    "        targets = targets.cuda()\n",
    "    print(trains.shape)\n",
    "    print(targets.shape)\n",
    "    y_train_pred = model(trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa73415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
